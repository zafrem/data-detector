# RAG Security Integration Guide

This guide provides a comprehensive overview of how to use Data Detector to protect Retrieval-Augmented Generation (RAG) systems from leaking Personally Identifiable Information (PII).

## Overview

Retrieval-Augmented Generation (RAG) systems are powerful, but they introduce significant security risks. The pipeline—which involves processing user queries, retrieving documents, and generating responses with an LLM—has multiple points where sensitive data can be inadvertently exposed:

-   **User Queries**: Users might include their own or others' PII in their prompts.
-   **Retrieved Documents**: The documents fetched from your vector database to provide context to the LLM might contain sensitive customer data.
-   **LLM Responses**: The LLM, in its attempt to be helpful, might synthesize and expose PII from the context in its final response.

Data Detector provides a **three-layer security architecture** specifically designed to mitigate these risks at every stage of the RAG pipeline.

## Core Concepts: The Three-Layer Security Architecture

Our security model is built on three layers of defense, each targeting a specific stage of the RAG pipeline.

```
┌─────────────────────────────────────────────────────────┐
│                    RAG PIPELINE                         │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  User Query                                             │
│       │                                                 │
│       ▼                                                 │
│  ┌──────────────────────────────────────┐              │
│  │  LAYER 1: INPUT BLOCKING             │              │
│  │  • Scan queries for PII              │              │
│  │  • Block or sanitize before RAG      │              │
│  │  • Prevent sensitive queries         │              │
│  └────────────┬─────────────────────────┘              │
│               │                                         │
│               ▼                                         │
│  ┌──────────────────────────────────────┐              │
│  │  Vector DB / Document Retrieval      │              │
│  └────────────┬─────────────────────────┘              │
│               │                                         │
│               ▼                                         │
│  ┌──────────────────────────────────────┐              │
│  │  LAYER 2: STORAGE BLOCKING           │              │
│  │  • Scan docs before indexing         │              │
│  │  • Tokenize PII (reversible)         │              │
│  │  • Store sanitized in vector DB      │              │
│  └────────────┬─────────────────────────┘              │
│               │                                         │
│               ▼                                         │
│  ┌──────────────────────────────────────┐              │
│  │  LLM Processing                      │              │
│  └────────────┬─────────────────────────┘              │
│               │                                         │
│               ▼                                         │
│  ┌──────────────────────────────────────┐              │
│  │  LAYER 3: OUTPUT BLOCKING            │              │
│  │  • Scan LLM responses                │              │
│  │  • Block leaked PII                  │              │
│  │  • Protect end users                 │              │
│  └────────────┬─────────────────────────┘              │
│               │                                         │
│               ▼                                         │
│  Safe Response to User                                  │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

1.  **Layer 1: Input Sanitization**: Scans incoming user queries to detect and handle PII *before* it is processed or logged.
2.  **Layer 2: Secure Storage**: Scans documents *before* they are indexed into your vector database. It can sanitize the content and, unique to this layer, replace PII with reversible tokens.
3.  **Layer 3: Output Filtering**: Scans the final response generated by the LLM *before* it is sent to the user, acting as a final line of defense against data leakage.

## Quick Start

This example demonstrates how to use the `RAGSecurityMiddleware` to apply all three layers of protection.

### Installation
```bash
pip install data-detector
```

### Basic Usage
```python
import asyncio
from datadetector import Engine, load_registry
from datadetector.rag_middleware import RAGSecurityMiddleware

async def main():
    # 1. Initialize the security middleware
    registry = load_registry()
    engine = Engine(registry)
    security = RAGSecurityMiddleware(engine)

    # 2. Layer 1: Scan the incoming user query
    query = "What's the email for john@example.com?"
    query_result = await security.scan_query(query, namespaces=["comm"])

    if query_result.blocked:
        print("Query was blocked because it contained sensitive content.")
        return

    # Use the sanitized query for the rest of the RAG pipeline
    sanitized_query = query_result.sanitized_text
    print(f"Sanitized Query: {sanitized_query}")

    # 3. Layer 2: Scan a document before indexing it in a vector database
    document_text = "The customer's email is john@example.com, and their SSN is 123-45-6789."
    doc_result = await security.scan_document(
        document_text,
        use_tokenization=True # Use tokenization to allow for future reversal
    )

    # Store the sanitized version of the document
    print(f"Sanitized Document: {doc_result.sanitized_text}")
    # vector_db.add(doc_result.sanitized_text)

    # The token_map allows you to reverse the process if needed. Store it securely.
    if doc_result.token_map:
        print(f"Token Map: {doc_result.token_map}")
        # secure_storage.save(doc_id, doc_result.token_map)

    # 4. Layer 3: Scan the LLM's response before sending it to the user
    llm_response = "Certainly, the customer's SSN is 123-45-6789."
    output_result = await security.scan_response(llm_response)

    if output_result.blocked:
        print("LLM response was blocked because it contained sensitive information.")
        return

    print(f"Final (Safe) Response: {output_result.sanitized_text}")

asyncio.run(main())
```

## REST API Reference

If you are running Data Detector as a standalone server, you can access the RAG security features through a set of dedicated REST API endpoints.

First, start the server:
```bash
data-detector serve --port 8080
```

### Layer 1: Scan Query
-   **Endpoint**: `POST /rag/scan-query`
-   **Usage**: To sanitize or block PII in user prompts.

### Layer 2: Scan Document
-   **Endpoint**: `POST /rag/scan-document`
-   **Usage**: To sanitize or tokenize PII in documents before indexing.

### Layer 3: Scan Response
-   **Endpoint**: `POST /rag/scan-response`
-   **Usage**: To filter or block PII in the LLM's final output.

For detailed request and response formats, please see the `API Reference` documentation.

## Integration Examples

The `RAGSecurityMiddleware` can be easily integrated into popular LLM frameworks.

### LangChain Integration
```python
from langchain.chains import RetrievalQA
from datadetector.rag_middleware import RAGSecurityMiddleware
# ... (imports and initialization)

security = RAGSecurityMiddleware(engine)

async def secure_qa_chain(query: str) -> str:
    """A wrapper for a LangChain query that includes PII protection."""
    # Layer 1: Scan the input
    input_result = await security.scan_query(query)
    if input_result.blocked:
        return "[Query blocked due to sensitive content]"

    # Pass the sanitized query to the chain
    response = await qa_chain.ainvoke(input_result.sanitized_text)

    # Layer 3: Scan the output
    output_result = await security.scan_response(response['result'])
    if output_result.blocked:
        return "[Response blocked due to sensitive content]"

    return output_result.sanitized_text
```

### FastAPI Integration
```python
from fastapi import FastAPI, HTTPException
from datadetector.rag_middleware import RAGSecurityMiddleware
# ... (imports and initialization)

app = FastAPI()
# Initialize the middleware once at startup and attach to app state
app.state.security = RAGSecurityMiddleware(engine)

@app.post("/chat")
async def chat_endpoint(query: str):
    """A secure chat endpoint that protects the RAG pipeline."""
    security = app.state.security

    # Layer 1: Input protection
    input_result = await security.scan_query(query)
    if input_result.blocked:
        raise HTTPException(status_code=400, detail="Query contains sensitive information that cannot be processed.")

    # ... (your RAG logic using input_result.sanitized_text) ...
    response_from_llm = await my_rag_pipeline(input_result.sanitized_text)

    # Layer 3: Output protection
    output_result = await security.scan_response(response_from_llm)
    if output_result.blocked:
        # You might want to return a generic message instead of an error
        return {"response": "I am sorry, but I cannot provide that information."}

    return {"response": output_result.sanitized_text}
```

## Security Policies

You can customize the behavior of each security layer by defining a `SecurityPolicy`. This allows you to, for example, be lenient with user inputs but very strict with LLM outputs.

| Action | Behavior | Recommended Use Case |
|---|---|---|
| `BLOCK` | Rejects the operation and returns a blocked status. | Best for the **output layer** to prevent any possibility of a leak. |
| `SANITIZE` | Removes or masks PII and allows the operation to continue with the safe text. | Best for the **input layer** and **storage layer** (without tokenization). |
| `WARN` | Logs a warning but allows the original text through. | Useful for monitoring in trusted internal environments. |

## Best Practices

-   **Use Layer-Specific Policies**: Your security posture should be different at each layer.
    -   **Input Layer**: Be more lenient. Sanitize PII but don't block queries unless they contain highly critical data (like passwords). This allows users to ask questions naturally.
    -   **Storage Layer**: Use `SANITIZE` with `use_tokenization=True`. This protects the data at rest while giving you a path to retrieve the original data if needed.
    -   **Output Layer**: Be as strict as possible. Use `BLOCK` to ensure that even if the LLM hallucinates or reconstructs PII, it never reaches the end-user.

-   **Store Token Maps Securely**: The `token_map` returned by `scan_document` is extremely sensitive as it contains the original PII. It should *never* be stored alongside the sanitized document. Store it in a separate, encrypted, and access-controlled database (like HashiCorp Vault or a secure secrets manager).

-   **Monitor and Alert on Detections**: Log every time PII is detected, especially if it's blocked. This can provide valuable insights into how users are interacting with your system and help you identify potential security weaknesses.

-   **Use Targeted Namespaces**: Don't scan for every possible PII type in every request. If you are building a chatbot for a US-based audience, you probably don't need to scan for Korean Resident Registration Numbers. Limiting the namespaces (`--ns` flag) significantly improves performance.

## Performance Optimization

-   **Use Streaming for Large Documents**: For indexing large document collections, the `StreamEngine` can process multiple documents concurrently, providing a significant speed-up.
-   **Stop on First Match**: If you only need to know *if* PII exists (and not what all of it is), use `stop_on_first_match=True`. This is much faster.
-   **Cache Patterns**: The `RAGSecurityMiddleware` is designed to be initialized once and reused. The expensive work of compiling patterns happens at startup, so avoid re-initializing it on every request.

## Troubleshooting

-   **High Latency?** Reduce the number of namespaces you are scanning, and use `stop_on_first_match=True` if possible.
-   **False Positives?** If the detector is incorrectly flagging data, consider creating more specific custom patterns for your domain or adjusting the `severity_threshold` in your security policy to be less strict.
-   **Token Maps Not Being Returned?** Ensure you have set `use_tokenization=True` in your call to `scan_document`.